---
layout:     post
title:      scrapy
subtitle:   scrapy爬取静态网页
date:       2019-01-25
author:     LY
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - scrapy
    

---

### 安装scrapy

最简便的方法，使用Anaconda3安装，在Anaconda Prompt里使用。

### 基本步骤

第一步：利用shell查看网页是否友好

第二步：先创建框架蜘蛛文件到桌面，将模板代码放到蜘蛛文件中

第三步：在shell中调试获得正确的xpath语句

第四步：将正确的xpath语句贴入模板对应部分

第五步：修改item名称为表格列名

第六步：运行文件，在cmd中输入 scrapy crawl 蜘蛛名Scrapy crawl 蜘蛛名 –o 名字1.csv

	from scrapy import Request
	from scrapy.spiders import Spider
	from wangzuxian.items import WangzuxianItem

	class Wangzuxian1Spider(Spider):
    	name = 'wangzuxian1'
    	start_urls = ['https://movie.douban.com/celebrity/1166896/movies?sortby=time/']

    	def parse(self, response):
         	item = WangzuxianItem()
         	dakuangjia = response.xpath('//div[@class="grid_view"]/ul/li')
         	for xiaokuangjia in dakuangjia:
             	item['title'] = xiaokuangjia.xpath('.//dl/dd/h6/a//text()').extract_first()
             	item["director"] = xiaokuangjia.xpath('.//dl/dd/dl//text()').extract()[3]
             	item["year"] = xiaokuangjia.xpath('.//dl/dd/h6/span[1]//text()').extract_first()
             	item["grade"] = xiaokuangjia.xpath('.//div[@class="star clearfix"]/span[2]//text()').extract_first()
             	item["partner"] = xiaokuangjia.xpath('.//dl/dd/dl//text()').extract()[7]
             	item["access"] = xiaokuangjia.xpath('.//div[@class="star clearfix"]/span[3]//text()').extract_first()
             	yield item

         	next_url =response.xpath('//a[text()="后页>"]/@href').extract_first()
         	absolute_next_page_url = response.urljoin(next_url)
         	yield Request(absolute_next_page_url)

### 实战

例如我现在要爬取大众点评日本菜的数据。第一步，在Anaconda Prompt里输入

	scrapy shell http://www.dianping.com/shanghai/ch10/g113
	view(response)

显示页面不友好，打开新建的scrapy startproject文件里的settings，把ROBOTSTXT_OBEY = True改为False，不遵守该协议。在下面框架里

	DEFAULT_REQUEST_HEADERS = {
	'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8','Accept-Language': 'en',
	}

添加'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'

在network-doc-request headers下可以找到。然后重新开始第一步，

	scrapy shell -s USER_AGENT='Mozilla/5.0' http://www.dianping.com/shanghai/ch10/g113
	view(response)


response.xpath('//*[@id="content"]/div/div[1]/ol')大框架


response.xpath('//*[@id="content"]/div/div[1]/ol/li[1]/div')小框架








